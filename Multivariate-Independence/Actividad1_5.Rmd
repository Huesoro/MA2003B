---
title: "Actividad1_5"
output: pdf_document
date: "2024-09-30"
---

```{r}
X = matrix(c(1, 4, 3, 6, 2, 6, 8, 3, 3), nrow = 3, ncol = 3, byrow = TRUE)
X
```

## a) Hallar la media, varianza y covarianza de X
Media

```{r}
medias <- colMeans(X)
medias
```
Varianza
```{r}
varianzas <- apply(X, 2, var)
varianzas
```
Covarianza
```{r}
covarianza <- cov(X)
covarianza
```
## b) Hallar la media, varianza y covarianza de b'X y c'X
Se crea b y c
```{r}
b <- matrix(c(1, 1, 1), nrow = 1, ncol = 3, byrow = TRUE)
c <- matrix(c(1, 2, -3), nrow = 1, ncol = 3, byrow = TRUE)
```
Se crea b'x y c'x, y se guarda la Y transpuesta
```{r}
bx <- b %*% t(X) 
cx <- c %*% t(X)
Y <- t(rbind(bx,cx))
Y
```

Obtener la media
```{r}
medias_cx <- colMeans(Y)
medias_cx
```
Obtener la varianza
```{r}
varY <- apply(Y, 2, var)
varY
```
## c) Hallar el determinante de S (matriz de var-covarianzas de X)
```{r}
S <- det(cov(X))
S
```
## d) Hallar los valores y vectores propios de S
```{r}
eigen(cov(X))
```

## e) Argumentar si hay independencia entre b'X y c'X , ¿y qué ocurre con X1, X2 y X3? ¿son independientes?

Dado que el determinante es 0, sabemos que existe dependencia lineal entre las variables $Y_1$ y $Y_2$. 

## f) Hallar la varianza generalizada de S. Explicar  el comportamiento de los datos de X basándose en los la varianza generalizada, en los valores y vectores propios de S.

```{r}
print("La varianza generalizada")
print(S)
print("Eigen valores y vectores")
print(eigen(cov(X)))
```
La varianza generalizada nos da una idea de la dispersión conjunta de todas las variables, como el determinante es 0, esto indica que las variables están correlacionadas linealmente y que existe dependencia lineal entre ellas.

Los valores propios de $S$ explican la varianza en las direcciones principales. Dados que los vectores son cercanos a cero, indican poca variación lo que refuerza la idea de dependencia lineal entre las variables.

# 2. Explore los resultados del siguiente código y dé una interpretación 
```{r}
library(MVN)
x = rnorm(100, 10, 2)
y = rnorm(100, 10, 2)
datos = data.frame(x,y)
mvn(datos, mvnTest = "hz", multivariatePlot = "persp")
mvn(datos, mvnTest = "hz", multivariatePlot = "contour")
```
Segun el valor $p=0.8130>0.05$ entonces, no hay suficiente evidencia para rechazar la hipotesis nula de Henze-Zirkler, por lo que los datos son consistentes para una distribución multivariada. De igual manera obtenido valores de $p>0.05$ para el test de Anderson-Darling en cada variable,lo cual significa que no hay suficiente evidencia para demostrar que los datos no siguen una desviación estandar, por lo que las variables siguen una distribución estandar. 
Los gráficos nos ayudan a visualizar como se distribuyen los datos en 3d, asi como en sus capas de nivel

# 3. Un periódico matutino enumera los siguientes precios de autos usados para un compacto extranjero con edad medida en años y precio en venta medido en miles de dólares. 
```{r}
x1 <- c(1,2,3,3,4,5,6,8,9,11)
x2 <- c(18.95, 19.00, 17.95, 15.54, 14.00, 12.95, 8.94, 7.49, 6.00, 3.99)
```

## a) Construya un diagrama de dispersión 
```{r}
plot(x1, x2, 
     main = "Diagrama de dispersión: Edad del auto vs Precio",
     xlab = "Edad del auto (años)", 
     ylab = "Precio de venta (miles de dólares)", 
     pch = 19, 
     col = "blue")
```

## b) Inferir el signo de la covarianza muestral a partir del gráfico. 

La covarianza será negativa, pues mientras la edad de los carros aumenta, el precio disminuye.


## c) Calcular el cuadrado de las distancias de Mahalanobis
```{r}
# Calcular el vector de medias
media <- colMeans(datos)

# Calcular la matriz de covarianza
S <- cov(datos)

# Calcular la distancia de Mahalanobis
distancias_mahalanobis <- mahalanobis(datos, center = media, cov = S)

# Mostrar las distancias de Mahalanobis
distancias_mahalanobis
```

## d) Usando las anteriores distancias, determine la proporción de las observaciones que caen dentro del contorno de probabilidad estimado del 50% de una distribución normal bivariada. 
```{r}
umbral_50 <- qchisq(0.5, df = 2)


observaciones_dentro <- sum(distancias_mahalanobis <= umbral_50)

# Calcular la proporción de observaciones dentro del contorno del 50%
proporcion <- observaciones_dentro / length(distancias_mahalanobis)
proporcion
```
## e) Ordene el cuadrado de las distancias del inciso c y construya un diagrama chi-cuadrado
```{r}
distancias_ordenadas <- sort(distancias_mahalanobis)

# Crear los cuantiles teóricos de la distribución chi-cuadrado
cuantiles_teoricos <- qchisq(ppoints(length(distancias_ordenadas)), df = 2)


qqplot(cuantiles_teoricos, distancias_ordenadas, 
       main = "Diagrama Q-Q: Distancias de Mahalanobis vs Chi-Cuadrado",
       xlab = "Cuantiles teóricos Chi-Cuadrado",
       ylab = "Distancias de Mahalanobis al cuadrado",
       pch = 19, col = "blue")
abline(0, 1, col = "red") 
```

## f) Dados los resultados anteriores, serían argumentos para decir que son aproximadamente normales bivariados?
Dado que las distancias de Mahalanobis siguen una distribución chi-cuadrado en el gráfico Q-Q.Asi como el 53% de los datos es parte del contorno del 50%. Podemos afirmar que los resultados son normales bivariados.

# 4. Ejercicio 4
Si X es un vector aleatorio con X1, X2, X3 son tres variables conjuntamente normales, no independientes, con b, un vector de 3 constantes, b1, b2 y b3, y c, otro vector de 3 constantes, c1, c2, c3, demuestra que las variables V1 = b'X y V2 = c'X son independientes si b´c = 0.

Dado que $X$ es un vector aleatorio normal multivariado, entonces cualquier combinación lineal de sus componentes también es una variable normal. Por lo que $V_1$ y $V_2$ son normales.

Dado que si no existe correlación entre las variables esto será independencia, entonces 

Hay que conseguir la covarianza entre $V_1$ y $V_2$
$$cov(V_1,V_2)=Cov(b'X,c'X)=b'Cov(X) c$$
Dado que la matriz de covarianzas es simétrica, este producto debe dar 0. Una vez que vemos que las variables son conjuntamente normales, y la covarianza entre $V_1, V_2$ es cero,  Esto demuestra que son independientes


Bien se sabe que la matriz de covarianzas es simétrica, por lo que el producto $b^TCov(X)c$ debe dar 0. Recordando que las variables son conjuntamente normales, y observando que la covarianza entre $V_1, V_2$ es cero, hemos demostrado que son independientes.



